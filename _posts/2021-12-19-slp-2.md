---
layout: post
title: SLP
date: 2021-12-19 10:19
author: admin
comments: true
categories: [看书]
---
<!-- wp:heading -->
<h2>2.正则表达式、文本规范化、编辑距离</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>文本规范化：从文本中分离出单词token，词形还原lemma（sang-&gt;sing），提取词干stem，文章划分segmentation</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>正则表达式 regular expression re：</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>正则表达式是文本中搜索字符串的语言。可以在<a href="https://tool.oschina.net/regex/" data-type="URL" data-id="https://tool.oschina.net/regex/">这个网站</a>上在线测试</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>基本：abc s12。re区分大小写</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>[]：析取，如[aAbB]bc，能查abc,Abc,bbc,Bbc。加横线表示范围，[a-zA-Z]能表示一个字母</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>[]内第一个字符是^时，表示取反，如[^a]bc,能查abc之外的字符串，Abc等等。注意当^在[]内其他位置时，只表示^这个符号，如[a^]bc，能匹配^bc</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>？：前面的字符可以有0个or1个。a?bc可以匹配abc和bc</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>*：前面的字符可以有任意个。a*bc可以匹配bc,abc,aa...bc。[ab]*可以匹配零个或多个a或b，比如ababb</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>整数字符串为[0-9][0-9]*，至少要有1位</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>+：前面的字符可以有1个或多个。[0-9]+也表示整数字符串</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>*和+默认尽可能匹配最大的字符串（贪心），*？和+？强制非贪心匹配。[a-z]+? 匹配所有小写字母，不匹配单词</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>定位符：^abc仅匹配行首的abc。abc$仅匹配行尾的abc。 ^The dog is mine$仅匹配The dog is mine这行. \b匹配空格，\B匹配非空格，a\b 可以匹配 a  ，不匹配Invade</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>abc|def：析取，abc或def。gupp(y|ies)匹配两个单词。注意[abcdef]会匹配这六个字符里的一个字符</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\d：一位数字 </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\D：一位非数字</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\w：一位字母/数字</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\W：一位非字母/数字，如！</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\s：空格</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\S：非空格</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>.：任意字符</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>{n,m}前面的字符出现至少n次，至多m次。没有n表示没有下限，没有m表示没有上限。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>前瞻断言：文本中向前看，但不移动光标，以便处理文本</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>单词</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>语料库corpus 语段utterance：差不多就是句子</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>词元lemma：单词去掉前后缀得到词根  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>类型type：语料库中不同单词的数量  符记token：语料库中所有单词的数量</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>文本规范化（预处理）</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>包括三个步骤：</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>1.tokenization分词</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>常用的英语分词标准：Penn Treebank，将附着词分隔开（doesn't-&gt;does n't），分隔标点符号</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>分词器归纳一组token，包含比单词更小的字符，称为子单词subword。现代分词器得到的token大部分是单词，有些是经常出现的subword，如er。得到subword是为了将没见过的单词分解为见过的subword。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>字节对编码算法（BPE）是一种分词方案。</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>将语料库的单词分解为单个字符，包括单词结尾_，放进词汇表</li><li>选择最经常相邻的两个符号连起来，在词汇表中添加这个新的符号。比如'A'和'B'最经常相邻，在词汇表中添加‘AB’</li><li>重复第二步，直到完成k个合并。k是参数</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>在测试集上，将单词分解为字母，按词汇表的顺序进行合并，如将e r替换为er，将er _替换为er_等等。k往往很大，大部分单词将表示为完整的符号，很少单词需要用subword来表示。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>2.规范单词格式</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>大小写（可能有害：US与us）、词形还原（时态、单复数等）</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>词干提取算法：Porter stemmer</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>按照一定的规则级联（重写的输出作为下一次重写的输入）运行</li><li>规则举例<ul><li>ational-&gt;ate如relational-&gt;relate</li><li>ing-&gt;null</li><li>sses-&gt;ss</li></ul></li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>3.分割句子</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>重点是确定标点符号是单词的一部分还是句子边界标记（如U.S.）。可以借助字典来判断。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>最小编辑距离</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>最小编辑距离是衡量两个字符串差异程度的度量，定义为将一个字符串替换为另一个字符串所需的最小编辑次数（插入、删除和替换）。进一步，可以为每个操作分配一个权重。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>最小编辑距离可以用动态规划来实现。因为若 A-&gt;C-&gt;B有最小的距离，A-&gt;C和C-&gt;B也是最小的，否则可以更改路径减小A-&gt;C或C-&gt;B的编辑距离，从而减小A-&gt;B的编辑距离。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>将D[i,j]定义为X[1,i]到Y[1,j]的编辑距离。边界为D[0,j]=j，D[i,0]=i</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>递推方程D[i,j]=min(D[i-1,j]+增加cost,D[i,j-1]+删除cost,D[i-1][j-1]+替换cost if X[i]!=Y[j] else D[i-1][j-1]+0)。</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>3.n-gram模型</h2>
<!-- /wp:heading -->

<!-- wp:heading {"level":3} -->
<h3>1.n-gram模型推导</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>任务：在给定历史h下，计算下一个单词是w的概率。P(w|h)=P(hw)/P(h)。hw可能不在训练集中，所以将h表示为w1w2..wn，P(hw)=P(w1)P(w2|w1:1)...P(wn|w1:n-1)P(w|w1:n)。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>直觉上，通过最后几个单词就能估计概率。比如bigram，把条件概率中的之前的单词组替换为上一个单词，P(wn|w1:n-1)=P(wn|wn-1)。bigram的假设也叫马尔科夫假设。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>推广到n-gram，上n-1个单词决定概率，P(wk|wk-(n-1):wk-1)。代入到P(w|h)就得到了n-gram模型。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>在语料库中计数，用频率估计这些概率（好听的说法是最大似然估计MLE）。这里有个trick，bigram时，$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)}=\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>具体使用bigram时，需要在句子的开头和结尾放上标注符号&lt;s&gt;,&lt;/s&gt;，为第一个单词提供前一个单词、最后一个单词提供后一个单词，进而为预测开头和结尾的词提供方便。使用n-gram时，添加n-1个&lt;s&gt;和&lt;/s&gt;。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>在得到下一个单词概率后将概率最大的单词添加到原句之后，再次预测下一个单词，直到得到&lt;/s&gt;，也就完成了续写句子。从&lt;s&gt;开始，能得到完全由模型生成的句子。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>实践中常用trigram，即3-gram。如果训练数据有很多，也可以考虑4-gram。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>概率的乘法通常用对数概率的加法来表示，因为概率乘法可能导致概率过小，数字下溢。直到最后使用概率结果时才指数化为概率。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>评估语言模型</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>外部评估：将模型用到实际的任务，看看是否真的有效。外部评估的缺点是太慢了。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>内部评估：将数据分为训练集-验证集-测试集，用一些指标来衡量模型。困惑度perplexity是常用的指标。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>句子W=w1w2...wN的困惑度$PP(W=w_1w_2…w_N)=P(w_1w_2…w_N)^{-\frac{1}{N}}=P(\Pi w_i|w_1…w_{i-1})^{-\frac{1}{N}}$。利用bigram，$PP(W)=P(\Pi w_i|w_{i-1})^{-\frac{1}{N}}$句子的条件概率乘积越高，困惑度就越低，模型就越好。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>困惑度的直观含义是加权的可能出现的下一个单词的数量。考虑0-9随机出现的字符串，很难预测下一个单词是什么，计算困惑度为10.如果知道了数字出现的概率，就能把困惑度降下来。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>注意，困惑度与词典密切相关（词典提供了条件概率），两种模型在使用同一词典的情况下才能比较。困惑度的改善不保证外在评估的改善。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>泛化与平滑</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>如果测试句子有一个没见过的单词，那么该单词条件概率是0，导致整个句子的概率也是0。进一步地，句子的困惑度无法计算。这种未知单词叫做表外词（OOV）。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>为了避免这种概率=0的情况发生，可以从频繁发生的事件中匀一些给没见过的事件。这种修改叫做平滑。另一种方式是回退，在高阶n-gram没找到的情况下用低阶n-gram替代。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>1.拉普拉斯平滑 每个计数都加1</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>2.加k平滑 每个计数都加k，k是个小数</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>3.插值平滑 n-gram用n到1-gram的线性组合替代。系数可以用EM算法求。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>4.Katz回退与good-turing</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5645,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/11/16381899361.png" alt="" class="wp-image-5645"/><figcaption>r是w1...wn出现的次数</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>dr是折扣因子，将高阶n-gram的概率打折。由good-turing，</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5647,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/11/16381903551-1.png" alt="" class="wp-image-5647"/><figcaption>r是w1...wn出现的次数，Nr是出现r次的n元语法数目</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>剩下的部分分配给回退的语言模型上.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>α是回退系数，保证概率和为1</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5648,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/11/16381906041.png" alt="" class="wp-image-5648"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>解得</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5649,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/11/16381906341.png" alt="" class="wp-image-5649"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>4.Kneser-Ney平滑，是最常用的n-gram平滑方法</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>考虑Hong Kong经常出现，测试Peking Kong的概率时，没有Peking Kong这个词，所以回退到Peking和Kong这两个比较高的概率，给出的概率也会比较高。但是，Kong脱离Hong单独使用的情况很少见，必须考虑到这点。https://www.jianshu.com/p/044108934966</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>5.stupid backoff</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><a href="https://zhuanlan.zhihu.com/p/341412531">https://zhuanlan.zhihu.com/p/341412531</a></p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>4.朴素贝叶斯与文本分类</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>文本分类可以用在情感分析、垃圾邮件检测、确定文本作者等等任务上。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>朴素贝叶斯</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>贝叶斯公式：$P(x|y)=\frac{P(y|x)P(x)}{P(y)}$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>要求X中给定y后概率最大的x<sub>i</sub>，P(y)都一样，所以$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>\hat{x_i}=argmax_{x_i\in X} P(x|y)=argmax_{x_i\in X} P(y|x)P(x)=argmax_{x_i\in X} P(y_1y_2…y_n|x_i)P(x_i)$。把$P(y_1y_2…y_n|x)$叫做似然度（likelihood），P(x)叫先验概率（prior probability）</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>这个公式也很难算，再利用朴素贝叶斯假设，认为P(y<sub>i</sub>|x)是独立的，因此$P(y_1y_2…y_n|x_i)=P(y_1|x_i)P(y_2|x_i)...P(y_n|x_i)$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>和语言模型一样，一大串的概率乘法都转化为对数。所以朴素贝叶斯公式为$\hat{x_i}=argmax_{x_i \in X}[log(P(y_1|x_i))+log(P(y_2|x_i))…log(P(y_n|x_i)) +log(P(x_i)) ]$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>像朴素贝叶斯这样输入的线性组合完成分类的分类器叫线性分类器。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>朴素贝叶斯的训练</strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>就是找两个概率：P(x<sub>i</sub>)与P(y<sub>j</sub>|x<sub>i</sub>)。最直接的想法是用训练集中的频率来表示概率（最大似然估计），但和n-gram一样需要做平滑，削去高概率的x<sub>i</sub>给没有出现过的x<sub>k</sub>。通常用拉普拉斯平滑就行：</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>$P(y_i|x)=\frac{count(y_i,x)+1}{\sum_{(y\in Y)}(count(y,x)+1)}=\frac{count(y_i,x)+1}{\sum_{(y\in Y)}count(y,x)+|Y|}$|Y|是训练集的总单词数。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>如果测试集中遇到不在词汇表中的词，直接忽略掉。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>nlp中更常见的写法是把条件y写作c，要预测的事件写作w</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5711,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16382913191.png" alt="" class="wp-image-5711"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>朴素贝叶斯可以很好地用于文本分类，但小小的改进可以提高性能。一个想法是，对许多文本分类任务，一个单词是否出现比出现频率更重要，所以把每个句子中重复的词都只保留一个。这种方法叫二进制多项式朴素贝叶斯。当然，出现在多个句子中的词汇记多次。另一个想法是，否定词可以完全改变一个句子的情感。在文本规范化过程中，在否定词后面的每个单词都加前缀NOT_，直到标点符号。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>实例：http://studyai.site/2015/12/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(03)%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9E%84%E5%BB%BA%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%E7%B3%BB%E7%BB%9F/</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>朴素贝叶斯怎么做情感分类？例子：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5718,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16383241671.png" alt="" class="wp-image-5718"/></figure>
<!-- /wp:image -->

<!-- wp:image {"id":5719,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16383241791.png" alt="" class="wp-image-5719"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>分类问题的评估，precision、recall、F-measure，见<a href="http://rojic12138.com/jiqixuexidaolun/#2%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F">机器学习导论笔记</a>。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>统计显著性检验</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>怎么比较两个分类器模型孰优孰劣？零假设与p值是个好办法。但不能用t统计量来估计p值，因为t统计量要求变量正态分布。在NLP中，基于采样使用非参数测试，常用bootstrap。配对测试可以直观比较两组观察值。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>bootstrap是从测试集中多次重抽样（抽过的还能再抽），得到b组数据。每一组也有多个数据，计算模型A和B的准确率（或其他度量标准），两者之差δ。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5742,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16383283321.png" alt="" class="wp-image-5742"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>零假设：A不比B好。预期总体δ(x)=0. 如果观察到的δ(x)很大的话说明零假设错误。因此可以计算p值：</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>$p-value(x)=\frac{1}{b}\sum_{i=1}^b \mathbb{I} (\delta (x^i)-\delta(x)&gt;=\delta(x))=<br>\frac{1}{b}\sum_{i=1}^b \mathbb{I} (\delta (x^i)&gt;=2\delta(x))$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>分类需要避免代表性伤害，比如将特定族裔与情感绑定。一个例子是互联网中有许多黑人负面的话，训练的结果很可能给黑人这个词很强的负面判断。</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>5.逻辑回归</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>逻辑回归也是一种有监督分类算法。它是神经网络的基础。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>分类器有两种：生成模型和判定模型。以分辨一张图是猫还是狗为例，生成模型根据猫的特征学习出一个猫的模型，根据狗的特征学习出一个狗的模型，然后从这张图中提取特征，放到猫的模型中看概率是多少，放到狗的模型中看概率是多少，哪个大就是哪个。朴素贝叶斯是生成模型。判定模型找特征来区分这两个类，比如说猫耳朵朝上，狗耳朵朝下。逻辑回归是判定模型。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>从概率分布的角度考虑，对于一堆样本数据，每个均有特征Xi对应分类标记yi。<strong>生成模型</strong>：学习得到<strong>联合概率分布</strong>P(x,y)，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。<strong>判别模型</strong>：学习得到<strong>条件概率分布</strong>P(y|x)，即在特征x出现的情况下标记y出现的概率。数据要求：生成模型需要的数据量比较大，能够较好地估计概率密度；而判别模型对数据样本量的要求没有那么多。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>逻辑回归有两个阶段：训练（随机梯度下降与交叉熵损失）与测试。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>测试</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>对一个观测值x，用特征向量(x1,x2...xn)来表示。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>计算z=w*x+b，w是权重向量。b是偏差项，是个实数。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>y=σ(z)，σ是Sigmoid函数，又叫逻辑函数（这也是逻辑回归名字得来）。$y=\sigma(z)=\frac{1}{1+e^{-z}}$。得到的y是介于0和1之间的数，可以表示概率。根据y与0.5的大小做判断，如果y=P(y=1|x)&gt;0.5则认为是。这里的0.5是<strong>决策边界</strong>decision boundary。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7372,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image.png" alt="" class="wp-image-7372"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>特征向量依赖于直觉与观察。比如情感分类，可以选取积极词数量、消极词 数量 、否定词、感叹号、句长为特征。许多任务需要大量的特征，我们会在之后几章讨论<strong>表示学习</strong>，以自动搜集特征。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>逻辑回归在较大数据量下比朴素贝叶斯效果更好，可以做NLP问题的baseline。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>训练</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p><strong>交叉熵</strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>训练模型时，通常考虑的不是相似度，而是距离。逻辑回归的损失函数是交叉熵，之后会看到，神经网络的损失函数也是交叉熵。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>首先将预测结果表示为$p(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$（花点时间想想，为什么是这样的）</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>两边取对数$logp(y|x)=ylog\hat{y}+(1-y)log(1-\hat{y})$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>这个式子描述了相似性，交叉熵只需要取相反数即可：</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>L<sub>CE</sub> $(\hat{y},y) =- [ylog\hat{y}+(1-y)log(1-\hat{y})]$ </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>随机梯度下降</strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>模型参数有w和b。机器学习中参数通常用θ表示。训练的目的是平均损失函数最小，也即：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7381,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-1.png" alt="" class="wp-image-7381"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>梯度下降法是找出函数梯度（方向导数最大的向量），往相反方向移动，移动的步长称为<strong>学习率</strong>η。也可以理解为各参数沿着损失函数对该参数的偏导的反方向移动 η 。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>交叉熵的梯度很好算，利用σ函数性质，$\frac{\partial L_{CE}(\hat{y},y)}{\partial w_j}=(\hat{y}-y)x_j$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>逻辑回归的损失函数通常是凸的，只有一个最小值；神经网络的损失函数则是非凸的，可能陷入局部最小值。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5775,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16384633631.png" alt="" class="wp-image-5775"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>随机梯度下降中的随机指的是一次只选择部分训练集而非全部。一次一个数据可能导致不稳定的移动（不具有代表性），所以通常使用mini-batch方法，一次并行处理一批。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>逻辑回归的mini-batch很简单，只需要求梯度的平均值就行。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>为了防止<strong>过拟合</strong>，一般用L2<strong>正则化</strong>（岭ridge回归）：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":5776,"sizeSlug":"large","linkDestination":"none"} -->
<figure class="wp-block-image size-large"><img src="http://82.157.22.122/wp-content/uploads/2021/12/16384636371.png" alt="" class="wp-image-5776"/><figcaption>方便求导，比L1更好用</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>或者L1正则化（套索lasso回归）</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7389,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-2.png" alt="" class="wp-image-7389"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>花点时间想想，为什么限制θ大小能防止过拟合？因为Θ越大，模型就越复杂，而真实模型往往是较为简单的。比如一些二次函数生成的散点，如果用三次函数拟合，可能得到完美拟合但很奇怪的模型。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>多项逻辑回归</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>前面讲了二分类问题。多分类时，σ函数就不够用了，需要使用softmax函数，将多个值映射为(0,1)之间的数。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7391,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-3.png" alt="" class="wp-image-7391"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>多项逻辑回归的测试没什么特别，选softmax后最高的值就行。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>训练要复杂一些，交叉熵$L_{CE}(\hat{y},y)=-\sum_{k=1}^Ky_klog\hat{y}_k=-\sum{k=1}^Ky_klog\hat{p}(y=k|x)$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>注意到只有一个y<sub>k</sub>=1，其余都=0，于是$L_{CE}(\hat{y},y)=-log\hat{p}(y=k|x)=-log\frac{e^{w_kx+b_k}}{\sum_i^K e^{w_ix+b_i}}$</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>梯度计算和二项逻辑回归类似</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7394,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-4.png" alt="" class="wp-image-7394"/><figcaption>1函数：y=k则是1，否则是0</figcaption></figure>
<!-- /wp:image -->

<!-- wp:heading -->
<h2>6.向量语义vector semantics与词嵌入word embedding </h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>什么是相似的词？从词本身出发，可以讨论词元、词根、前后缀等等。从词环境出发，可以比较上下文。猫和狗不是同义词，但却是相似词——把一句话的狗换成猫，不会有什么违和感。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>vector semantics是NLP中表示词的标准方法，能用空间中的一个点来表示单词，根据词在空间中的分布可以判断词的关系。表示词的向量叫做embedding。这节介绍两种模型：PPMI模型和word2vec模型，分别产生稀疏的、密集的向量。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7396,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-5.png" alt="" class="wp-image-7396"/></figure>
<!-- /wp:image -->

<!-- wp:heading {"level":3} -->
<h3>PPMI 模型</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>词-词矩阵：每个单元格表示行词和列词在上下文中一起出现的次数。上下文通常是一个窗口，比如左边四个词，右边四个词。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7397,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-6.png" alt="" class="wp-image-7397"/><figcaption>窗口</figcaption></figure>
<!-- /wp:image -->

<!-- wp:image {"id":7398,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-7.png" alt="" class="wp-image-7398"/><figcaption>词-词矩阵</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>因为词向量的长度是一样的（都是总词数|V|），词向量之间的相似度用余弦相似度度量。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7399,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-8.png" alt="" class="wp-image-7399"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>问题：good、bad、the、he这些词出现频率很高，意义却不大，所以我们想剔除这些常用词。类似<a href="http://rojic12138.com/hulianwangshujuwajue/">数据挖掘</a>中tf-idf文档-词矩阵的方法，用PPMI来处理词-词矩阵。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>目标词w和上下文词c的<strong>逐点互信息</strong>$PMI(w,c)=log_2\frac{P(w,c)}{P(w)P(c)}$。分子是两者共现的概率。PMI的值从负到正无穷（想想为什么），负值（共现频率比独立分布更低）往往是不可靠的，需要语料库非常庞大，比如概率为10<sup>-6</sup>的两个词独立分布，同时出现的概率是10<sup>-12</sup>。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>PPMI是对PMI的改进：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7402,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-9.png" alt="" class="wp-image-7402"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>举例：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7404,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-10.png" alt="" class="wp-image-7404"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>计算概率：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7405,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-11.png" alt="" class="wp-image-7405"/></figure>
<!-- /wp:image -->

<!-- wp:image {"id":7406,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-12.png" alt="" class="wp-image-7406"/><figcaption>概率矩阵</figcaption></figure>
<!-- /wp:image -->

<!-- wp:image {"id":7407,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-13.png" alt="" class="wp-image-7407"/><figcaption>PPMI矩阵</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>PPMI对异常词的改进：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7408,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-14.png" alt="" class="wp-image-7408"/><figcaption>想想为什么设置小于1的α有效（比较熟悉大于1的α会增大频繁词的概率）</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>求得词向量后，可以求文档的质心：所有词的平均向量。根据文档的质心可以用余弦相似度求文档相似性。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>Word2Vec模型</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>PPMI得到的向量长度是|V|，包含所有词，这个向量太长而且相当稀疏。密集向量的表现往往更好，因为要学习的权重少很多而且过拟合较好。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Word2Vec的直觉是预期计算w在c附近出现的概率，不如计算分类器来预测“w是否可能在c附近出现？”我们实际并不关心预测结果，只把分类器权重作为embedding。skip-gram是 Word2Vec 的一个实现，步骤为：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7411,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-15.png" alt="" class="wp-image-7411"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>为每个单词学习两个embedding：target embedding和context embedding，两者加在一起得到词的embedding。embedding的长度是超参数，一般设为300.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>损失函数-交叉熵：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7412,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-16.png" alt="" class="wp-image-7412"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>训练过程：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7413,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-17.png" alt="" class="wp-image-7413"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>Word2Vec是一种静态embedding方式，得到向量就不再变化。之后看到的BERT得到的向量在不同的上下文中时不同。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>embedding的语义：研究发现，窗口较小时，相似度最高的词往往是语义相似的词；窗口较大时，相似度最高的词往往是主题相似的词。比如Hogwarts，窗口较小时相似度最高的是其他虚构学校，窗口较大时是哈利波特的主题词。</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>7.神经语言模型</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>神经网络的知识跳过不讲，请看我的<strong><a href="http://rojic12138.com/jiqixuexidaolun/">机器学习笔记</a></strong>。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>神经语言模型就是用神经网络来预测下一个单词。比n-gram有更高的预测精度。它的输入是embedding。举个例子，4-gram用前三个词来预测后一个词，神经语言模型的输入就是前三个词的embedding。</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7418,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-18.png" alt="" class="wp-image-7418"/><figcaption>2层网络（不计算输入层）</figcaption></figure>
<!-- /wp:image -->

<!-- wp:image {"id":7419,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-19.png" alt="" class="wp-image-7419"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>训练：不仅训练神经网络的W、U，还训练词的embedding。在计算图上误差反传计算梯度，随机梯度下降，交叉熵，这些没什么特别的。</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>8.词类POS的序列标注sequence labeling </h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>词类POS和专有名词是了解句子结构和意义的有用线索。POS有8种：名词、动词、代词、介词、副词、连词、分词和冠词。为输入单词序列的每个单词xi分配标签yi，使得输出序列Y与输入序列X具有相同长度的任务称为序列标注。本节介绍两种序列标注法：生成式的隐马尔科夫模型HMM和区分式的条件随机场CRF。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>可以将POS分为两大类：封闭类与开放类，前者在语法中有固定用法，后者则常常被创造出来。开放类包括名词、动词、形容词、副词和感叹词。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>隐马尔科夫模型HMM</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>马尔科夫假设：预测未来时，过去都不重要，只有现在重要。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>隐马尔科夫模型是概率有向图，有五个参数：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7425,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-20.png" alt="" class="wp-image-7425"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>HMM可以解决评估、解码、学习：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7427,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-21.png" alt="" class="wp-image-7427"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p><a href="https://zhuanlan.zhihu.com/p/166646231">https://zhuanlan.zhihu.com/p/166646231</a></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>概率图求解：维特比算法，分层动规</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>条件随机场CRF</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>设计一组特征函数集，对句子的不同词性序列进行评价，对每个序列求和，再转化为概率，以概率最高的序列为准。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>特征函数举例：当l_i是“副词”并且第i个单词以“ly”结尾时，我们就让f1 = 1，其他情况f1为0。不难想到，f1特征函数的权重λ1应当是正的。而且λ1越大，表示我们越倾向于采用那些把以“ly”结尾的单词标注为“副词”的标注序列。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><a href="https://www.zhihu.com/question/35866596/answer/139485548">https://www.zhihu.com/question/35866596/answer/139485548</a></p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>9.循环神经网络RNN和Transformer</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>如何用神经网络来处理NLP问题？我们之前讲过embedding，将词变为长度一样的向量，考虑n-gram，尝试用前面三个词的embedding作为输入，所有词的概率作为输出。这样的网络问题在于首先窗口大小有限，其次网络的学习模式也不好，比如“all the ”这两个词出现在第一、二个位置和第二、三个位置需要让网络学习两种模式，但其实是一样的。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>困惑度perplexity</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>如何评价语言模型的好坏？希望从句子本身出发，越像人话——概率越高，困惑度越低。句子越长，概率越低，所以指数要带上N。</p>
<!-- /wp:paragraph -->

<!-- wp:image -->
<figure class="wp-block-image"><img src="https://www.zhihu.com/equation?tex=PP%28W%29%3DP%28w_%7B1%7Dw_%7B2%7D...w_%7BN%7D%29%5E%7B-%5Cfrac%7B1%7D%7BN%7D%7D%3D%5Csqrt%5BN%5D%7B%5Cfrac%7B1%7D%7BP%28w_%7B1%7Dw_%7B2%7D...w_%7BN%7D%7D%7D" alt="[公式]"/></figure>
<!-- /wp:image -->

<!-- wp:heading {"level":3} -->
<h3>循环神经网络</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>与普通的神经网络相比，循环神经网络的隐层指向自己，每次计算隐层时会考虑隐层上一个时间点的值。注意，隐层中一个神经元受隐层中所有神经元的影响，而不止是它自己。只有一个隐层的情况：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7433,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-22.png" alt="" class="wp-image-7433"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>前向传递：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7434,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-23.png" alt="" class="wp-image-7434"/><figcaption>g是激活函数，f是softmax</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>因为隐层中的值是从句子开始继承的，所以没有窗口大小限制。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>RNN的训练看似比较难，因为隐层导致的损失不仅体现在这一时刻的输出，还体现在下一时刻的隐层以及输出，以及之后时刻。但实际操作和普通神经网络没什么区别，把上一个时刻的隐层看成这一个时刻输入层的一部分即可照常训练：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7437,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-24.png" alt="" class="wp-image-7437"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>基于RNN的语言模型（输入层只包含一个单词的情况）：</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>以句子开头&lt;s&gt;作为第一个输入，得到第一个单词</li><li>以第一个单词的embedding作为输入，得到第二个单词</li><li>以第二个单词的embedding作为输入，得到第三个单词</li><li>直到得到句子结束&lt;/s&gt;或达到句长限制。</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p><strong>端到端训练</strong>end to end training：忽略系统的不同阶段，使用单个神经网络代替。比如情感分类，一个句子只有最后的一个输出，之前的输出对训练调参都没有用。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>多层RNN</strong>：RNN的输出序列可以作为另一个RNN的输入序列：</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":7439,"sizeSlug":"full","linkDestination":"none"} -->
<figure class="wp-block-image size-full"><img src="http://82.157.22.122/wp-content/uploads/2022/01/image-25.png" alt="" class="wp-image-7439"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p><strong>双向RNN</strong>：普通RNN是从前往后的，双向RNN新加了一个从后往前的RNN。在预测任务中，对两个RNN给出的值进行组合（逐元素加或乘），得到最后结果。比如“我吃了_浇饭”，两个RNN分别使用“我吃了”和“浇饭”来给出概率分布，相加得到双向RNN的概率分布。</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>长短时记忆LSTM和门控单元GRU</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>RNN的隐层往往是相当局部的，不能解决遥远信息问题。LSTM能解决长序列训练中梯度消失和梯度爆炸问题。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>参考<a href="https://zhuanlan.zhihu.com/p/32085405">https://zhuanlan.zhihu.com/p/32085405</a></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>与普通RNN相比多了一个更新较快的层。</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>LSTM内部有三个阶段：忘记、选择记忆和输出。</p>
<!-- /wp:paragraph -->

<!-- wp:image -->
<figure class="wp-block-image"><img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_720w.jpg" alt=""/><figcaption>z都是h+x经过矩阵运算得来的</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>GRU比LSTM更简单，效果相似。</p>
<!-- /wp:paragraph -->

<!-- wp:image -->
<figure class="wp-block-image"><img src="https://pic3.zhimg.com/80/v2-5b805241ab36e126c4b06b903f148ffa_720w.jpg" alt=""/><figcaption>用z同时实现遗忘和记忆</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>之前我有疑问：LSTM和GRU是怎么实现遗忘和记忆的呢？虽然有网络节点，但这些节点是怎么掌握这个功能的呢？其实这些节点的连接本身没有不同，和普通的神经网络一样训练就行。</p>
<!-- /wp:paragraph -->
